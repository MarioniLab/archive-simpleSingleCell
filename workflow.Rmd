---
title: A worfklow for low-level analyses of single-cell RNA-seq data
author: 
    - name: Aaron T. L. Lun
      affiliation: Cancer Research UK Cambridge Institute, Li Ka Shing Centre, Robinson Way, Cambridge CB2 0RE, United Kingdom
    - name: John C. Marioni
      affiliation: Cancer Research UK Cambridge Institute, Li Ka Shing Centre, Robinson Way, Cambridge CB2 0RE, United Kingdom; EMBL European Bioinformatics Institute, Wellcome Genome Campus, Hinxton, Cambridge CB10 1SD, United Kingdom
date: 19 February 2016
vignette: >
    %\VignetteIndexEntry{A worfklow for low-level analyses of single-cell RNA-seq data}
    %\VignetteEngine{knitr::rmarkdown}
output: 
    BiocStyle::html_document:
        fig_caption: yes
bibliography: ref.bib
---

```{r style, echo=FALSE, results='hide', message=FALSE}
library(BiocStyle)
library(knitr)
opts_chunk$set(error=FALSE)
opts_chunk$set(fig.width=7, fig.height=7)
opts_chunk$set(dpi=300, dev="png", dev.args=list(pointsize=15))
```

```{r, message=FALSE, echo=FALSE, results='hide'}
library(scran)
library(DESeq2)
library(edgeR)
```

# Introduction

Single-cell RNA sequencing (scRNA-seq) is widely used to measure the genome-wide expression profile of individual cells.
From each cell, mRNA is isolated and reverse transcribed to cDNA for high-throughput sequencing [@stegle2015computational].
This can be done using microfluidics platforms like the Fluidigm C1 [@pollen2014lowcoverage], or with protocols based on microtiter plates like Smart-seq2 [@picelli2014fulllength].
The number of reads mapped to each gene can then be used to quantify its expression in each cell.
Alternatively, unique molecular identifiers (UMIs) can be used to directly measure the number of transcript molecules for each gene [@islam2014quantitative].
Count data can be analyzed via dimensionality reduction and clustering to identify new cell subpopulations, and to detect highly variable genes (HVGs) or differentially expressed genes (DEGs) between subpopulations or conditions.
This provides biological insights at a single-cell resolution that cannot be attained with conventional bulk RNA sequencing of cell populations.

Strategies for scRNA-seq data analysis differ markedly from those for bulk RNA-seq.
One technical reason is that scRNA-seq data is much noisier than bulk data [@brennecke2013accounting;@marinov2014singlecell].
Reliable capture (i.e., conversion) of transcripts into cDNA for sequencing is difficult with the low quantity of RNA in a single cell.
This increases the high frequency of drop-out events where none of the transcripts for a gene are captured.
More PCR amplification cycles are often used to compensate for low input quantities, but this can lead to amplification biases and inflated measures of transcript levels.
Dedicated analysis steps are required to deal with this noise, especially during quality control.
In addition, scRNA-seq data can be used to study cell-to-cell heterogeneity, e.g., to identify new cell subtypes, to characterize differentiation processes, to separate cells based on cell cycle phase, or to identify HVGs driving variability across the population [@vallejos2015basics;@fan2016characterizing;@trapnell2014dynamics].
This is simply not possible with bulk data, such that custom methods are required to perform these analyses. 

This article describes a computational workflow for basic analysis of scRNA-seq data using software packages from the open-source Bioconductor project [@huber2015orchestrating].
Starting from a count matrix, this workflow contains the steps required for quality control to remove problematic cells; normalization of cell-specific biases, with and without spike-ins; cell-cycle phase classification from gene expression data; data exploration to identify putative subpopulations; and finally, HVG and DEG identification to prioritize interesting genes.
The application of different steps in the workflow will be demonstrated on several public scRNA-seq data sets -- one from a study of cell types in the mouse brain, and another from a study of mouse embryonic stem cells (mESCs) cultured under different conditions [@kold2015singlecell].
The aim is to provide a variety of modular usage examples that can be applied to construct custom analysis pipelines.

# A simple analysis on haematopoietic stem cells

## Overview

To introduce most of the concepts of scRNA-seq data analysis, we use a relatively simple data set from a study of haematopoietic stem cells (HSCs) [@wilson2015combined].
Mouse HSCs were purified and 96 cells were sequenced using the Smart-seq2 protocol.
A constant amount of spike-in RNA from the External RNA Controls Consortium (ERCC) was also added to each cell prior to library preparation.
The expression of each gene was quantified by counting the total number of reads mapped to the exonic regions of that gene.
Similarly, the quantity of each spike-in transcript was measured by counting reads mapped to the spike-in reference sequence.
Counts for all genes/transcripts in each cell were obtained from the NCBI Gene Expression Omnibus as a supplementary file under the accession number GSE61533.

For simplicity, we forgo a description of the read processing steps required to generate the count matrix, i.e., read alignment and counting into features.
These steps have been described in some detail elsewhere [@love2015rnaseq], and are largely the same for bulk and single-cell data.
The only additional consideration is that the spike-in information must be included in the pipeline.
Typically, spike-in sequences can be included as additional FASTA files during genome index building prior to alignment, while genomic intervals for both spike-in transcripts and endogenous genes can be concatenated into a single GTF file prior to counting.
For users favouring a R-based approach to read alignment and counting, we suggest using the methods in the `r Biocpkg("Rsubread")` package [@liao2013subread;@liao2014featurecounts].

## Count loading and quality control

The first task is to load the count matrix into memory.
This requires some work to decompress and retreive the data from the Excel format.
Each row of the matrix represents an endogenous gene or a spike-in transcript, and each column represents a single HSC.

```{r}
library(R.utils)
gunzip("GSE61533_HTSEQ_count_results.xls.gz", remove=FALSE)
library(gdata)
all.counts <- read.xls('GSE61533_HTSEQ_count_results.xls', sheet=1, header=TRUE, row.names=1)
dim(incoming)
```

Rows corresponding to spike-in transcripts are identified by the `ERCC` prefix in the row names.
For convenience, the counts for spike-in transcripts and endogenous genes are stored separately in a `SummarizedExperiment` object.
This is done using the `countsToSE` function in the `r Rpackage("scran")`package.

```{r}
library(scran)
spike.in <- grepl('^ERCC', rownames(all.counts))
y <- countsToSE(all.counts[!spike.in,], all.counts[spike.in,])
sum(spike.in)
```

Low-quality cells are defined as those where the endogenous RNA has not been efficiently captured (i.e., converted into cDNA and amplified) during library preparation.
These can be identified as those cells with low library sizes, i.e., a low sum of counts for the endogenous genes.
In general, any cell with a library size that is an order of magnitude lower than the median across all cells can be considered to be of low quality and removed.

```{r}
keep <- y$lib.size >= median(totals)/10
y <- y[,keep]
sum(keep)
```

Another measure of low quality involves the proportion of reads mapped to genes in the mitochondrial genome.
High proportions are indicative of poor-quality cells [@islam2014quantitative], possibly because of increased apoptosis.
Here, the mitochondrial genes have been conveniently marked with a `mt-` prefix, so the proportion can be easily computed for each cell.

```{r mitoplot, fig.cap="Histogram of the proportion of reads mapped to mitochondrial genes across all cells."}
is.mito <- grepl("^mt-", rownames(y))
mito.props <- colSums(assay(y)[is.mito,])/y$lib.size 
hist(mito.props, xlab="Mitochondrial proportion", ylab="Number of cells", 
    breaks=20, cex.lab=1.4, cex.axis=1.2, main="")
```

The appropriate threshold on the mitochondrial proportion will vary from case to case, depending on the cell type and the experimental protocols.
If we assume that most cells in the data set are not apoptotic, then the threshold can be set to remove obvious outliers from the distribution of proportions.
A threshold value of 0.09 is probably suitable for this data set, based on the previous histogram.

```{r}
keep <- mito.props < 0.09
y <- y[,keep]
sum(keep)
```

Very low-abundance genes are also removed prior to further analysis.
These genes tend to provide little information as the counts are too low for reliable inferences.
In addition, the discreteness of the counts may interfere with downstream statistical procedures, e.g., by compromising the accuracy of asymptotic approximations.
Here, low-abundance genes are defined as those with an average count across cells below 1.
Removing them avoids problems with discreteness and also reduces the amount of computational work.

```{r}
keep <- rowMeans(assay(y)) >= 1
y <- y[keep,] 
sum(keep)
```

## Normalization of cell-specific biases

Read counts are subject to differences in capture efficiency and sequencing depth between cells [@stegle2015computational].
Normalization is required to eliminate these cell-specific biases prior to downstream quantitative analyses.
This is often done by assuming that most genes are not differentially expressed (DE) between cells.
Any systematic difference in count size across the non-DE majority of genes between two cells is assumed to represent bias and is removed by scaling.
More specifically, "size factors" are calculated that represent the extent to which counts should be scaled in each library.

Size factors can be computed with several different approaches, e.g., using the `estimateSizeFactorsFromMatrix` function in the `r Biocpkg("DESeq2")` package [@love2014moderated], or with the `calcNormFactors` function [@robinson2010scaling] in the `r Biocpkg("edgeR")` package (this requires an additional multiplication by the library size).
However, single-cell data can be problematic for these bulk data-based methods due to the dominance of low and zero counts.
To overcome this, we pool counts from many cells to increase the count size for accurate size factor estimation [@lun2016much].
Pool-based size factors are then "deconvolved" into cell-based factors for cell-specific normalization.

```{r, warning=FALSE}
y$size.factor <- normalizeBySums(y, sizes=c(20, 40, 60, 80))
summary(y$size.factor)
```

Normalized log-expression values can be computed for use in downstream analyses.
Each value is defined as the log-ratio of each count to the size factor for the corresponding cell (after adding a small prior count to avoid undefined values at zero counts).
Division by the size factor ensures that any cell-specific biases are removed.
The log-transformation is also useful as larger counts can be modelled reasonably well with a log-normal distribution [@law2014voom].
The computed values are stored as an `"exprs"` matrix in the `SummarizedExperiment` object, in addition to the existing count matrix.

```{r}
y <- normalize(y)
assayNames(y)
```

## Data exploration with dimensionality reduction techniques

Dimensionality reduction is often useful to examine major features of the data before more quantitative analyses.
Of particular interest is whether the HSCs partition into distinct subpopulations.
This can be visualized by constructing a principal components analysis (PCA) plot from the normalized log-expression values.
Cells with more similar expression profiles should be located close together on the plot.

```{r pcaplot, fig.cap="PCA plot constructed from normalized log-expression values, where each point represents a cell."}
out <- prcomp(t(assay(y, "exprs")), scale.=TRUE)
plot(out$x[,1], out$x[,2], xlab="PC1", ylab="PC2", pch=16, 
    main="Expression values", cex.axis=1.2, cex.lab=1.4, main="")
```

Alternatively, the PCA plot can be constructed from a matrix of correlations between cells. 
Two cells with similar expression profiles will exhibit similar patterns of correlations to other cells.
As a result, those two cells will be closer together in the plot.
This approach tends to be more robust to noise and outliers that would dominate a standard PCA plot, at the cost of being less sensitive to differences in the expression profiles.

```{r pcacorplot, fig.cap="PCA plot constructed from a matrix of Spearman rank correlations between cells, where each point represents a cell."}
cor.mat <- cor(assay(y, "exprs"), method="spearman")
out <- prcomp(cor.mat)
plot(out$x[,1], out$x[,2], xlab="PC1", ylab="PC2", pch=16, main="Correlations")
```

Another popular approach to dimensionality reduction is the _t_-stochastic neighbour embedding (_t_-SNE) method [@van2008visualizing].
A fast implementation is provided by the `Rtsne` function in the `r CRANpkg("Rtsne")` package, and can be applied to the normalized log-expression values for all cells.
Note that, unlike PCA, _t_-SNE is a stochastic method -- users should run the algorithm several times to ensure that the results are representative, and then set a seed to ensure that the chosen results are reproducible.
It is also advisable to test different settings of the "perplexity" parameter as this will affect the distribution of points in the low-dimensional space.

```{r tsneplot, fig.cap="_t_-SNE plot constructed from normalized log-expression values, where each point represents a cell.", fig.width=12, fig.height=6}
library(Rtsne)
set.seed(100)
par(mfrow=c(1,3), cex.axis=1.2, cex.lab=1.3, cex.main=1.4)
transposed <- t(assay(y, "exprs"))
out <- Rtsne(transposed, perplexity=5)
plot(out$Y[,1], out$Y[,2], xlab="t-SNE1", ylab="t-SNE2", main="Perplexity=5", pch=16)
out <- Rtsne(transposed, perplexity=10)
plot(out$Y[,1], out$Y[,2], xlab="t-SNE1", ylab="t-SNE2", main="Perplexity=10", pch=16)
out <- Rtsne(transposed, perplexity=20)
plot(out$Y[,1], out$Y[,2], xlab="t-SNE1", ylab="t-SNE2", main="Perplexity=20", pch=16)
```

In general, _t_-SNE tends to work better than PCA for large data sets with complex substructure.
However, this comes at the cost of more computational effort and complexity.
For this particular data set, both methods suggest that there is no strong partionining into distinct subpopulations.
Of course, there are many dimensionality reduction techniques that we have not considered here but could also be used, e.g., multidimensional scaling.
In addition, greater resolution of differences can be achieved by applying these methods on expression values for a selected set of HVGs.
This focuses on genes that are driving heterogeneity and potential substructure. 

## 

```{r}
# Cell cycle phase classification.
mm.pairs <- readRDS(system.file("exdata", "mouse_cycle_markers.rds", package="scran"))
library(org.Mm.eg.db)
anno <- select(org.Mm.eg.db, keys=rownames(y), keytype="SYMBOL", column="ENSEMBL")
ensembl <- anno$ENSEMBL[match(rownames(y), anno$SYMBOL)]
keep <- !is.na(ensembl)
assignments <- cyclone(assay(y, "exprs")[keep,], mm.pairs, gene.names=ensembl[keep])
plot(assignments$score$G1, assignments$score$G2M)

g1.only <- assignments$score$G1 > 0
y <- y[,g1.only]

# Identifying highly variable genes.
var.fit <- trendVar(y, trend="loess")
var.out <- decomposeVar(y, var.fit)

# Identifying correlated genes.
set.seed(100)
top.hvgs <- order(var.out$bio, decreasing=TRUE)[1:200]
var.cor <- correlatePairs(y[top.hvgs,])

# Looking at the top genes.
heat.vals <- assay(y, "exprs")
heat.vals <- heat.vals - rowMeans(heat.vals)
is.sig <- var.cor$FDR <= 0.05 
chosen <- unique(c(var.cor$gene1[is.sig], var.cor$gene2[is.sig]))
require(gplots)
pdf("out.pdf", height=12, width=6)
heatmap.2(heat.vals[chosen,], col=bluered, symbreak=TRUE, trace='none', cexRow=0.5)
dev.off()
```

```{r}
incoming <- read.table('GSE60361_C1-3005-Expression.txt.gz', header=TRUE)
keep <- !duplicated(incoming[,1])
all.counts <- incoming[keep,-1]
rownames(all.counts) <- incoming[keep,1]

totals <- colSums(all.counts)
okay.libs <- totals > 1e4 
all.counts <- all.counts[,okay.libs]

library(scran)
y <- countsToSE(all.counts) 
clusters <- quickCluster(y)
y$size.factor <- normalizeBySums(y, cluster=clusters)
y <- normalize(y)

# Cell cycle phase classification.
mm.pairs <- readRDS(system.file("exdata", "mouse_cycle_markers.rds", package="scran"))
library(org.Mm.eg.db)
anno <- select(org.Mm.eg.db, keys=rownames(y), keytype="SYMBOL", column="ENSEMBL")
ensembl <- anno$ENSEMBL[match(rownames(y), anno$SYMBOL)]
keep <- !is.na(ensembl)
assignments <- cyclone(assay(y, "exprs")[keep,], mm.pairs, gene.names=ensembl[keep], BPPARAM=MulticoreParam(5))
plot(assignments$score$G1, assignments$score$G2M)

var.fit <- trendVar(y, trend="loess", use.spikes=FALSE)
var.out <- decomposeVar(y, var.fit)

set.seed(100)
top.hvgs <- order(var.out$bio, decreasing=TRUE)[1:200]
var.cor <- correlatePairs(y[top.hvgs,])

heat.vals <- assay(y, "exprs")
heat.vals <- heat.vals - rowMeans(heat.vals)
is.sig <- var.cor$FDR <= 0.05 
chosen <- unique(c(var.cor$gene1[is.sig], var.cor$gene2[is.sig]))
heatmap.2(heat.vals[chosen,], col=bluered, symbreak=TRUE, trace='none', cexRow=0.5)
```

# Choosing between normalization with and without spike-ins

## Overview
Scaling normalization strategies for scRNA-seq data can be broadly divided into two classes.
The first class assumes that there exists a subset of genes that are not differentially expressed between samples.
This subset can be manually specified to contain house-keeping genes, or it can be empirically identified under the assumption that most genes are not DE [@anders2010differential;@robinson2010scaling].
Any systematic difference in the counts across the non-DE subset is treated as technical bias and is eliminated by scaling.
The second class of normalization strategies uses spike-in RNA of known composition and abundance.
Specifically, the same quantity of spike-in RNA is added to each cell, captured into libraries and sequenced along with endogenous transcripts [@stegle2015computational].
Differences in the coverage of the spike-in transcripts can only be due to cell-specific biases, e.g., in capture efficiency or sequencing depth.
Scaling normalization is then applied to equalize spike-in coverage across cells.

The choice between these two normalization strategies depends on the biology of the cells and the features of interest.
If there is no reliable house-keeping set, and if the majority of genes are expected to be DE, then spike-in normalization may be the only option for removing technical biases.
Spike-in normalization should also be used if differences in the total RNA content of individual cells are of interest.
This is because the same amount of spike-in RNA is added to each cell, such that the relative quantity of endogenous RNA can be easily quantified in each cell.
For non-DE normalization, any change in total RNA content will affect all genes in the non-DE subset, such that it will be treated as bias and removed.
(This may be desirable if changes in total content are _not_ interesting.)
Similarly, if spike-ins are not present or if they cannot be added to each cell in a reliable manner, then non-DE normalization should be applied.

## Computing size factors for spike-in normalization

The use of spike-in normalization can be demonstrated on a simple data set comparing mESCs and mouse embryonic fibroblasts (MEFs) [@islam2011characterization].
A constant quantity of synthetic spike-in RNA was added to each cell, and spike-in transcripts were sequenced and counted along with transcripts from endogenous genes.
Quantification of gene expression was performed in this manner for 48 mESCs, 44 MEFs and 4 negative controls.
We obtain a table of read counts from NCBI GEO using the accession GSE29087, and load them into a `SummarizedExperiment` object for further manipulation.

```{r}
counts <- read.table("GSE29087_L139_expression_tab.txt.gz", skip=6, 
    sep='\t', row.names=1)[,-c(1:6)]
is.spike <- grepl("SPIKE", rownames(counts))
sum(is.spike)
library(scran)
y <- countsToSE(counts[!is.spike,], counts[is.spike,])
y$grouping <- factor(rep(c("mESC", "MEF", "Neg"), c(48, 44, 4)))
dim(y)
```

We remove low-quality libraries that have low total counts for the endogenous genes or high proportions of reads mapped to genes on the mitochondrial genome.
This removes a number of libraries, including those for the negative controls.
We also remove lowly expressed genes with an average count below 1.

```{r}
totals <- colSums(assay(y))
is.mito <- grepl("^mt-", rownames(y))
okay.libs <- totals >= 1e5 & colSums(assay(y)[is.mito,])/totals < 0.1 
y <- y[,okay.libs]
sum(okay.libs)
keep.gene <- rowMeans(assay(y)) >= 1
y <- y[keep.gene,] 
sum(keep.gene)
```

We compute size factors using the `normalizeBySpikes` function.
The size factor for each cell is proportional to the total number of reads mapped to spike-in transcripts.
The aim is to normalize the counts such that the total spike-in coverage is the same across cells.
These values are stored in the `SummarizedExperiment` object as previously described, and downstream analyses can be applied.

```{r}
sf.spike <- normalizeBySpikes(y)
y$size.factor <- sf.spike
````

Note that mESCs have consistently larger size factors compared to MEFs.
This is due to the fact that the former contain substantially less endogenous RNA than the latter [@islam2011characterization].
Larger size factors result in smaller normalized expression values for mESCs compared to MEFs, reflecting the decrease in total RNA content in the former.
These differences are lost when a normalization method based on a non-DE majority is applied, such as that in the `r Biocpkg("DESeq2")` package [@anders2010differential].

```{r}
y$grouping <- droplevels(y$grouping)
boxplot(split(sf.spike, y$grouping), log="y", cex.axis=1.2, cex.lab=1.4)
library(DESeq2)
y2 <- y
y2$size.factor <- estimateSizeFactorsForMatrix(assay(y2))
boxplot(split(y2$size.factor, y$grouping), log="y", cex.axis=1.2, 
    cex.lab=1.4, ylim=range(sf.spike))
```

## Effect of spike-in normalization on downstream analyses

Preservation of differences in total RNA content has some notable implications for downstream analyses.
For DEG detection between cell types, the change in total RNA content will be incorporated into the log-fold change.
This means that more DEGs are generally detected upon spike-in normalization compared to normalization based on a non-DE majority.
In addition, there tends to be a greater imbalance in the number of up- and downregulated genes, as the change in total RNA content occurs in one direction for all genes.
This is demonstrated below by comparing the DE results between the two normalization strategies. 

```{r}
library(edgeR)
d <- DGEList(assay(y, "counts"))
d$offset <- log(y$size.factor)
design <- model.matrix(~y$grouping)
d <- estimateDisp(d, design)
fit <- glmFit(d, design)
res <- glmLRT(fit)
summary(decideTestsDGE(res))
d2 <- d
d2$offset <- log(y2$size.factor)
d2 <- estimateDisp(d2, design)
fit2 <- glmFit(d2, design)
res2 <- glmLRT(fit2)
summary(decideTestsDGE(res2))
```

One might question the relevance of DE that is driven by changes in total RNA content between cell types.
Clearly, however, there will be a change in transcript abundance when the total amount of RNA in each cell changes.
Thus, from a technical perspective, the detection of DE for those genes is appropriate.
Biologically, the relevance of DE would depend on whether an increase in transcript molecules results in more molecular activity, e.g., due to increased production of protein product.
While this relation seems trivially true, it is easy to imagine situations where it is not the case, e.g., when the translation machinery is saturated such that a global increase in transcript molecules has no effect.

<!-- Also, if there's variability in cell sizes within groups, this'll reduce power to detect DEGs. -->

For HVG detection, any variability in total RNA content is incorporated into the variance of each gene.
This means that it will increase the size of the biological component of the variance.
However, the magnitude of the increase will be the same for each gene as the variance of RNA content is constant.
As such, even though the biological component will increase, the ranking of HVGs will be largely preserved. 

```{r}
y <- normalize(y)
trend <- trendVar(y, trend="loess", span=0.8)
components <- decomposeVar(y, trend)
y2 <- normalize(y2)
trend2 <- trendVar(y2, trend="loess", span=0.8)
components2 <- decomposeVar(y2, trend2)
```

Most dimensionality reduction and clustering procedures tend to be robust to the choice of normalization method.
This is because cells with different RNA content also tend to have different transcript compositions.
Thus, they would be (correctly) separated based on their gene expression profiles, regardless of whether changes in total RNA content were preserved after normalization.
Indeed, correlation-based methods are completely insensitive to normalization as the value of the correlation is not dependent on the scaling of expression values within each cell.

```{r, fig.width=12, fig.height=6}
par(mfrow=c(1,2))
col <- c("red", "blue")[y$grouping]
out <- prcomp(t(assay(y, "exprs")), scale.=TRUE)
plot(out$x[,1], out$x[,2], col=col, pch=16, xlab="PC1", ylab="PC2")
out2 <- prcomp(t(assay(y2, "exprs")), scale.=TRUE)
plot(out2$x[,1], out2$x[,2], col=col, pch=16, xlab="PC1", ylab="PC2")
```

## Classifying cells into cell cycle phases

## Detecting DEGs between pre-defined groups of cells

# Conclusions

# Software availability

```{r}
sessionInfo()
```

# Author contributions

A.T.L.L. developed the workflow on all data sets.

# Competing interests

No competing interests were disclosed.

# Grant information

CRUK core funding (SW73) to J.C.M.

# Acknowledgements

Aaron is pretty awesome.

# References

