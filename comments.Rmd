# Read alignment comments

## What to do about duplicate removal

Obviously, UMI counts already have duplicates removed, but what about read count-based methods?
It probably doesn't make much difference for SMART-seq2 and friends performing whole-transcript amplification.
This is because reads at different positions could be derived from amplified copies of the same transcript.
Duplicate removal based on genomic position only protects against PCR duplicates during library preparation.
See https://doi.org/10.1038/srep25533 for further details.

# Quality control comments

## Improving resolution with log-transformed QC metrics

By improving resolution, I refer to compression of high values and expansion of the range of low values.
The former reduces the MAD relative to the median, such that "3 MADs away" is a sensible statistic.
The latter makes it easier to distinguish between outliers and the edge of the distribution of acceptable values.
Or, from another perspective, the large MAD that is driven by greater variability at high values isn't relevant to the threshold choice at low values when working on the raw scale.
By transforming to the log scale, the variance is stabilised across the real line.

On a more conceptual note, the MAD is necessary to account for genuine biological heterogeneity in these metrics.
That's why we don't use a hard-and-fast fold-change threshold from the median, as this would be too aggressive or not aggressive enough in some situations.

## Interpreting the proportion mapped to spike-ins

It shouldn't matter too much if it's the proportion against total counts, or proportion against endogenous counts.
This is because we're not measuring an increase in mitochondrial/spike-in counts, but rather, a depletion of endogenous RNA.
If endogenous RNA decreases in low-quality cells, the mitochondrial/spike-in proportions against the total count should both increase.
We don't have to worry about effects of e.g. an increase in mitochondrial counts affecting the proportion of spike-in counts.

The absolute value of the spike-in proportion can also be used for QC.
You would want about 5-10% of the reads going to the spike-ins.
If this is not the case, it suggests that you need to alter the dilution.
You can also compare the observed proportions to the expected values, which can be calculated if RNA quantification was done on the cells beforehand.
Neither of these approaches provide a threshold for filtering, but they do tell you if the experiment went well or not.

Also, we don't use the logit transform for the proportions, even though on the raw scale we could theoretically end up with a above-unity threshold.
This is because the logit transform compresses changes within the middle of the [0,1] range.
This reduces the resolution for where the threshold would usually be.

## Assumptions of outlier identification

There's an implicit assumption that these technical metrics are homogeneous across cells.
That won't be true for extreme cases like erythrocytes, where the "outliers" would just be a genuine biological clustering.
It also won't be true for batches with known differences in experimental processing, e.g., sequencing at different depth or had different amounts of spike-in added.
This can be handled to some extent using the `block` argument in the `isOutlier` function.

# Cell cycle classification comments

## Relationship with filtering

It seems to more sense to apply `cyclone` before filtering out low-abundance genes.
This is because phase-specific genes that are not expressed in *any* cell will still be useful for classification.
Their lack of expression relative to other genes will make them informative pairs, so tossing them out would be counterproductive.

## Explaining poor performance on the brain data set

Another possible contributor to poor performance is the difference in the cells used for training and those in the test data.
If certain expression patterns are associated with the cell cycle in the training set (of mouse embryonic stem cells), these may be incorporated into the classifier.
However, if those patterns are not associated with the cell cycle in the test data, their inclusion will add noise without providing any phase information.
This will lead to a deterioration in the accuracy of the classifier.
In general, this is unlikely to be a major issue as the cell cycle should be a conserved process across many lineages and conditions.

Furthermore, there may be misclassification due to a large number of cells being in G0.
In theory, these should be closest to G1 but they may be different enough that you'd get a low G1 score, making them show up as S-like or even G2M.

# Filtering comments

## Justification in the context of HVG detection

It could be argued that you don't need to do filtering on abundance if you're going to select on HVGs anyway.
This is because the HVG screen would throw out low-abundance genes, so you might as well skip the abundance filter.
The disadvantage is if the HVG screen depends on significance, in which case the low abundances would increase the severity of the MTC.
This could be mild (~20% increase in the p-value) or quite severe (2-3-fold), depending on how many annotated genes are retained.
The worst case occurs if you had mild contamination so that every gene had a count of one - you'd retain too many uninteresting genes if you just filtered on non-zero totals.

To this end, the choice of filter is guided by the upper bound of the variance compared to the expected amount of technical noise.
The variance is bounded as the counts must be non-negative, so a gene with a low mean count can only achieve a particular variance (of the logs).
Below, we use linear programming to identify the maximum variance of logs at any specified mean of logs/mean of counts. 
We optimize over the proportions of cells with counts from 0 to 200.

```{r}
library(lpSolve)
counts <- 0:200
lmeanfun <- log2(counts+1)
meanfun <- counts

sumfun <- rep(1, length(counts))
sumval <- 1
posfun <- diag(length(counts))

collected <- list()
for (meanval in 1:100/100) {
    for (lmeanval in 1:100/100) {
        lvarfun <- (log2(counts+1) - lmeanval)^2
        out <- lp(direction="max", objective.in=lvarfun, 
                const.mat=rbind(meanfun, lmeanfun, sumfun, posfun), 
                const.dir=c("=", "=", "=", rep(">=", length(counts))),
                const.rhs=c(meanval, lmeanval, sumval, rep(0, length(counts))))
        if (out$objval==0) { next }
        collected[[length(collected)+1]] <- c(lmeanval, meanval, out$objval)
    }
}
collected <- do.call(rbind, collected)
```

In the HVG data set, an average count of 1 corresponds to a maximum variance of around 1.
The technical noise is around about this much anyway, so it makes no sense to keep genes with lower average counts, because they just wouldn't be called as HVGs.
We could be more stringent and require some minimum increase above the technical noise, but we won't do that.

```{r}
plot(collected[,1], collected[,3], xlim=c(0, 1), ylim=c(0, 1))
comp <- read.table("hsc_hvg.tsv", header=TRUE)
o <- order(comp$mean)
lines(comp$mean[o], comp$tech[o], col="red", pch=16, cex=0.5)
```

Similar logic applies in the brain data set.
Here, the technical noise is lower, so we set a smaller filter.
It depends somewhat on whether you want to extrapolate the trend with `rule=2` or as a line passing through the origin.
In both cases, though, it's safe to say that the maximum variances are comparable to the trend.

```{r}
plot(collected[,1], collected[,3],xlim=c(0, 1), ylim=c(0, 1), cex=ifelse(collected[,2] < 0.1, 1, 0))
comp <- read.table("brain_hvg.tsv", header=TRUE)
o <- order(comp$mean)
lines(comp$mean[o], comp$tech[o], col="red", pch=16, cex=0.5)
```

Note that directly filtering on the log-means is _technically_ correct, in that it's independent of HVG detection.
However, it's possible for genes to have very small log-means and very large variances, e.g., if you have a rare subpopulation with a very strongly expressed gene.
Filtering on the log-means would discard such genes, which we don't want to do.

Note that `technicalCV2` can reject the null at any average count.
This is because it is possible to obtain arbitrarily large CV^2^ values for infintesimally small means.
However, this mainly picks up genes with outlier expression profiles, which probably aren't interesting in most applications.

## Justification on other grounds

Genes with low counts may be due to transcriptional leakage or mapping/sequencing errors (especially pseudogenes).
This makes them uninteresting as they're unlikely to be related to any genuine biology.
One could even go so far as to say that genes that aren't expressed much are likely to be irrelevant to the phenotype.
Of course, there's likely to be a couple of low-abundance genes that (a) are important and (b) vary noticeably across cells.
However, this is probably the exception rather than the rule; getting hammered by the MTC and other statistical problems to squeeze these genes out seems suboptimal.

Note that this reasoning has nothing to do with the rectangular component on the plot.
That component just represents the bunch of genes with near-zero expression, as the log-transform spreads them out across a larger range for greater resolution. 
Thus, if you want to get rid of low-abundance genes, you should be setting your filter threshold somewhere within this component.
Otherwise, if it's too high, you might end up removing the bulk of moderately-expressed genes.

An additional motivation is to protect the normalization machinery from low-abundance genes.
Most ratio-based normalization methods will be messed up by low counts, because the count:mean ratios for low-abundance genes will be more variable (CV^2^ is higher).
Many methods also use a robust average like the median to approximate the mean count.
The median of the count:mean ratio should thus be near the true size factor, but this is only accurate for NB distributions with large means.
It could also be argued that you should do the analysis using only the genes used for normalization, lest there be other biases in the filtered genes.

In the deconvolution method, you can consider the threshold multiplied by the minimum pool size (20 cells by default) as the minimum pooled count.
This gives us pooled counts of around `20*1` for read data, which should be large enough to avoid the above problems for NB dispersions below 0.5.
For UMI data, we get pooled counts of `20*0.1` -- however, the variability is near-Poisson anyway, so variability and bias in the ratio should be okay.
In general, the maximum bias seems to be about 20%, which is probably tolerable if the data set is dominated by genes in the the high-abundance peak.

## Saving the original data

It's also good practice to save the full data set before filtering.
This is because the filtered genes in one context (e.g., for the full data set) might not be of the greatest interest in another context (e.g., with a subset of the data).
A prime example would be in iterative clustering, or when looking at subgroups.
In such cases, it's useful to filter and process everything afresh.

# Normalization comments

## Motivation for not using non-linear normalization

There are probably some non-linear biases in scRNA-seq data, i.e., beyond global scaling.
However, these are a pain to deal with because it's hard to fit a robust trend with respect to abundance.
Most robust methods (e.g., loess) rely on normality, and log-transforming counts becomes highly dependent on the pseudo-count at low counts.
Conversely, we could use discrete GLMs but that depends on proper specification of dispersions and is also less robust to outliers.

Another problem is that non-linear normalization assumes that most genes at each point of the covariate range are not DE.
This is reasonable in bulk data, but might not hold at the single-cell level.
Cell-to-cell heterogeneity and intra-cell correlations means that it is entirely possible that we get large-scale shifts in highly expressed genes.
For example, a subpopulation may upregulate a set of genes, resulting in a skew in a particular abundance category.
This would be eliminated upon normalization, which would not be ideal.

Finally, the non-linear effects seem relatively minor compared to other things, e.g., between-cell variability, plate effects.
Variance in the size factors due to non-linearity in the HSC data set is an order of magnitude smaller than variance in the size factors themselves.
It's probably worse between batches, but then you should be blocking on batch anyway.
The worst is likely observed DE analyses that are overpowered due to the large number of cells, but that has other problems...

```{r}
library(scran)
sce <- readRDS("hsc_data.rds")
ab <- calcAverage(sce)
bottom <- ab < median(ab)
summary(ab[bottom])
summary(ab[!bottom])
sf.low <- computeSumFactors(sce, subset.row=bottom, sizes=c(20, 40, 60, 80), sf.out=TRUE)
sf.hi <- computeSumFactors(sce, subset.row=!bottom, sizes=c(20, 40, 60, 80), sf.out=TRUE)
var(log10(sf.hi)) # 0.0161
var(log10(sf.hi/sf.low)) # 0.0013

sce2 <- readRDS("brain_data.rds")
clusters <- quickCluster(sce2)
ab2 <- calcAverage(sce2)
bottom2 <- ab2 < quantile(ab2, 0.75)
summary(ab2[bottom2])
summary(ab2[!bottom2])
sf.low2 <- computeSumFactors(sce2, subset.row=bottom2, cluster=clusters, sf.out=TRUE)
sf.hi2 <- computeSumFactors(sce2, subset.row=!bottom2, cluster=clusters, sf.out=TRUE)
var(log10(sf.hi2)) # 0.0941
var(log10(sf.hi2/sf.low2)) # 0.0082

# In comparison, the difference between size factors and library sizes
# still explains a fairly large proportion of the size factor variability.
var(log10(sizeFactors(sce2)/colSums(counts(sce2)))) # 0.0300
var(log10(colSums(counts(sce)))) # 0.0369
var(log10(sizeFactors(sce2))) # 0.1039
# Obviously doesn't happen in the HSC data, which is too homogeneous to care.
```

## Why use log-transformed normalized counts?

The log-transformation provides some measure of variance stabilization for NB-distributed counts with a constant dispersion but a variable mean. 
This seems better than square-rooting it, which is the VST for the Poisson only.
I guess you could play around with various Box-Cox transformations, but the mean-variance trend is pretty extreme and probably won't go away just by changing lambda.

## Dealing with dropouts 

The difficulty with dealing with dropouts is that you don't know whether a zero count is a true dropout or due to a genuine lack of expression.
It's possible to figure this out based on expression in other cells, e.g., a zero is more likely to be a dropout if expression is low in other cells.
However, this makes some assumptions, e.g., that the true expression is sampled from the same distribution for all cells.
Getting an actual value also requires that the true dropout rate for all genes follow the trend, which might not be true if you have gene-specific dropouts.

Putting this issue aside, there are several ways of handling dropouts on a per-observation basis - imputation, censoring or downweighting.
Imputing seems somewhat circular if you try to impute from the same data, rather than using some external data (e.g., in variant calling).
Censoring suspected dropouts avoids the need to guess what the non-zero value would be, though it results in loss of a proportion of genuine zeroes.
Downweighting seems most gradated but it depends on a sensible definition of the weights (which is not obvious) and the ability of downstream tools to use such weights.

Our approach is to handle the drop-outs on a per-gene basis, by modelling the technical variability introduced by them.
This is simpler but doesn't deal with situations where the dropout rate varies per cell due to changes in total RNA content.
(Note, not due to changes in sequencing depth or amplification, because these should not affect the capture rate.)
This is analogous to the difference between `voom` and _limma_-trend.

However, I guess that there mightn't be much to be concerned about.
This is because you can get a natural sigmoidal curve without requiring an explicit zero inflation term in a distribution.
Thus, scaling of the values would work just as well to account for changes in total RNA content.

```{r}
means <- 1:1000/10
disp <- 1
y <- matrix(rnbinom(100000, mu=means, size=1), nrow=length(means))
dropout <- rowSums(y==0)/ncol(y)
plot(means, dropout, log="x")
```

## Additional normalization for confounding effects

The percentage of variance explained by an uninteresting technical effect has obvious effects on HVG detection.
However, it also affects the correlation between genes because it represents some common underlying factor.
For a factor with increasing percentage explained 'p', Pearson's correlation will increase to 1, e.g., by 'p' at a true correlation of zero.
(This is based on adding Normal variates to each other, with one part representing the true expression of each gene and the other representing the common factor.
These two components are independent of each other within each gene -- you can then decompose the covariance between genes to get the correlation-proportion relationship.)

If 'p' is decently large (>10%), we're likely to have problems, so the corresponding factor will be need to be regressed out.
Of course, blocking on factors introduces more assumptions and points of failure to the analysis (e.g., linearity in the covariates).
If the model is misspecified, you can end up with spurious patterns in the residuals - possibly larger than that caused by the technical bias in the first place.
This is compounded by the presence of zeroes, non-normality and heteroskedasticity, etc.
Thus, blocking should be performed sparingly, rather than being performed by default.

## Misspecifying the model when running `removeBatchEffect`

Comparing residuals between blocking factor levels can run into problems if population structure varies between levels.
Imagine a case where we have two batches, NO batch effect and different proportions of the same cell types in each batch.
Computing residuals can result in spurious differences within each cell type for genes that are DE between cell types.
This is because the batch-specific average will be different due to the different composition of each batch.
It's actually worse than a completely confounding effect, as at least total confounding would just result in loss of differences and a false negative.

This seems to only affect situations where residuals need to compared across levels.
For variance calculations, residual effects are evaluated in terms of their total size, so this is less of an issue.
In fact, the whole point is to pick up highly variable genes that aren't captured well by the model, so this is a good thing.
For correlations with one-way layouts, comparisons are done within each level so it should be fine.
There are problems for additive designs, but we knew that already.

With all that being said, correction is probably the lesser of two evils if you have a strong batch effect that compromises the visualization.
It shouldn't matter for technical effects that are largely orthogonal to population structure (e.g., balanced designs).
Problems would only occur when you try to regress out uninteresting biological effects that might have some composition differences.

# HVG detection comments

## Trend fitting to variances of the log-counts

The mean-variance trend for log-expression values is more complex and difficult to fit than that of other approaches.
But we can do it, so it's a technical challenge rather than a philosophical one.

The thin line to the left is a mathematical artifact when you have discrete counts.
- the variance will increase as a concave quadratic when you can only choose between 0 and 1 (low 'p' increases with fixed 'n' in a binomial RV).
- the variance will increase as a convex quadratic with the mean, if variance is driven by an outlier and everything else is 0.

I'm not sure how much effort it's worth to extract HVG information from this part of the plot, as low counts are dominated by Poisson sampling noise.

## Motivating the threshold for significance

Specifically, for a standard normal, the square root of the expected squared distance between two cells would be sqrt(2). 
So, if you set the standard error to 1/sqrt(2), the distance would become 1 (i.e., 2-fold change).
Setting this threshold avoids selecting genes with high fold changes above the technical variance, but small absolute total variances.
Such genes are more likely to be true positives but also less likely to be strongly variable and biologically interesting.

(Of course, genes driving separation of rare subpopulations would have variances below 0.5.
Even if the log-fold changes between subpopulations is strong, the average squared difference across all cells would be low.
However, you've got to draw the line somewhere, otherwise you'll end up with lots of genes with really homogenous expression profiles, and irrelevant noise during PCA, etc.
I guess this is just the price that needs to be paid before you can go on and do clustering to explicitly identify subpopulations.)

The threshold is a bit informal, but that's okay.
The DM, for example, is no better, and Brennecke has that +0.25 value that isn't very interpretable).
In and of themselves, HVGs are not of interest -- rather, they prioritise genes for more interesting analyses, e.g., clustering, correlation and gene set analyses.
If HVG detection is considered as a screen, it is better to focus on potentially interesting (but possibly false) genes rather than true and uninteresting ones.

To this end, it is permissible to relax the FDR for detecting HVGs, especially if you didn't get anything interesting things with a low threshold.
Of course, this would also increase the amount of genes dominated by technical noise later on, so it's not preferable if you can avoid it.
One could argue that this is not problematic if you just get more low-variability genes, as these don't contribute much to relative differences betwen cells.
However, technical noise is still high (in absolute terms) and there are a lot more of them, so it would probably still mess up the results.

In any case, the p-values calculated here are probably more appropriate than those from Brennecke.
Log-expression values are a lot more normal-looking than the raw counts, due to the skew of the latter.

## Why not use PCA?

An alternative to feature selection by detecting correlated HVGs is to simply take the first set of PCs.
This will immediately summarize genes into correlated _and_ highly variable axes.
The problem is that it doesn't properly model technical noise.
The expression profile of every gene gets scaled to a unit variance, even if they are fully driven by technical noise.
Sure, noise should be random but it will still affect the quality of the PCs that you pull out.
(If you don't scale, then the selection of genes will be dominated by the mean-variance relationship.)

```{r}
par(mfrow=c(1,2))
loc <- 1:100/100 # True placement of cells
a1 <- matrix(jitter(rep(loc, 50)), nrow=50, byrow=TRUE) # Correlated genes
x1 <- prcomp(t(a1))
plot(x1$x[,1]) # Should be on the diagonal
a2 <- rbind(a1, matrix(rnorm(100000), ncol=100)) # Adding uncorrelated noise
x2 <- prcomp(t(a2))
plot(x2$x[,1]) # Correct placing is disrupted
```

A related problem is if the technical noise is correlated between genes, e.g., due to incomplete normalization of biases.
This would show up as an earlier PC if all genes are affected.
Formal selection of HVGs should reduce this problem by enriching for genes with biological variation.
The PCs are also less robust to outliers compared to the use of Spearman's rho.

Another consideration is interpretability.
The nature of HVGs and correlations is easy to understand and can be used in intermediate steps (e.g., to build regulatory networks, to compare heterogeneity).
This is harder to do with PCs, which are more abstract analytical entities.

## Pros and con of using log-count variances over CV^2^

Log-count variances are also more consistent with downstream applications.
For example, PCA and t-SNE are applied on the log-values, as is visualization of expression with boxplots or violin plots.
Making the latter with genes identified as HVGs from CV2 would give outliers that get shrunk upon log-transformation.
Indeed, the variance of the log-values provides a measure of the log-fold change between cells, which is arguably more relevant than the absolute differences in expression.

With CV^2^ you pick up a lot of genes expressed in few cells, which can be a pain for downstream analyses.
Most directly, it means that the HVG list isn't easily interpretable if you have to manually weed out a lot of uninteresting genes.
It also disrupts identification of trajectories, as well as clustering of large but subtle populations 
(i.e., subpopulations consisting of many cells, which are poorly separated from neighbouring subpopulations). 
This is because the relevant genes don't get picked up if the mean is decently large -- or, if they do get picked up, they get dominated by noise from outliers.
Of course, this is all relative -- for strong trajectories or clear subpopulations, both methods should perform well.

On the other hand, as a result of the robustness to outliers, detection power of the log-based method is reduced for HVGs driven by rare subpopulations.
It is difficult to detect these with the log-based method, given that uniquely expressed genes with a near-infinite fold-change are already hard to pick up. 
CV^2^-based methods do better, thought they are also less effective at detecting HVGs for subpopulations characterised by a loss of expression.
This is probably because the mean is already large, which limits the scale of the change in the CV^2^.

In short, the final recommendation is to use the log-based methods for initial exploration, because it's better at recovering major features in the data.
You can then switch to `technicalCV2` when pulling out rare subpopulations.
Also check out https://github.com/MarioniLab/MiscellaneousCode/tree/master/HVG2017 for simulation details used to get to the conclusions above.

## Biological interpretion of HVGs

We can interpret HVGs as genes where each cell has an (unknown) true expression that varies across cells, e.g., due to subpopulations or across a continuum.
This can also be extended across time, e.g., due to transcriptional bursting or circadian rhythms.
In other words, HVGs are equivalent to DE genes for unknown subsets of cells.
Validating whether the variability is functionally relevant becomes straightforward, as we can just KO or overexpress the gene.
This is equivalent to the strategy that would be used to validate the underlying DE, if the subsets were known.
One can also see this as seeing what happens after reducing the variance by coercing everyone to be lowly or highly-expressing.
While it won't preserve the population mean, this is largely irrelevant if HVGs are to equivalent to DEGs anyway.
(Such a task -- reducing variability while preserving the mean -- would be monumentally difficult.)

## Reasoning behind iterative HVG and clustering

The set of HVGs detected within a cluster may be more relevant.
This is because you can detect HVGs at greater power if you didn't have uninvolved, constantly-expressing cells dragging down the variance/correlations.
Similarly, you'd get rid of genes that are HVGs between clusters but are not within the cluster, which wouldn't help with internal clustering.

## Why spike-ins are detected as HVGs

A few spike-ins are detected as being highly variable, despite the fact that they should not exhibit any variability at all.
This seems to be caused by cell- and spike-specific amplification biases that introduce additional variability for a few spike-in transcripts.
(For example, if a transcript is hard to capture consistently but easy to amplify, it'll get a large mean and large variability.)
As a result, the total variance is increased above the technical trend, mimicking the effect of biological variance.
I think it's due to PCR because the fluctuations are less pronounced for UMI data.

```{r}
sce <- readRDS("hsc_data.rds")
out <- technicalCV2(sce, min.bio.disp=0)
out["ERCC-00108",]
plot(out$mean, out$cv2, log="xy", col=ifelse(out$FDR <= 0.05, "black", "grey"), pch=16)
points(out$mean, out$cv2, col="red", cex=ifelse(is.na(out$FDR), 1, 0), pch=16)

fit <- trendVar(sce, trend="semiloess", span=0.2, start=list(a=2, b=2, n=5))
dec <- decomposeVar(sce, fit)
dec["ERCC-00108",]
plot(dec$mean, dec$total, col=ifelse(dec$FDR <= 0.05, "black", "grey"), pch=16)
curve(fit$trend(x), col="blue", add=TRUE)
points(dec$mean, dec$total,  col="red", cex=ifelse(is.na(out$FDR), 1, 0), pch=16)
```

This affects all HVG detection methods, and it's unlikely that we can do much computationally to correct it, given that it's indistinguishable from genuine biological variance.
The CV^2^ method is more robust with the default `min.bio.disp`, which scales up the null CV^2^ to mitigate detection of spike-ins.
The 0.5 threshold for the log-based method is less protective, because the absolute biological component for the spike-ins is quite large.

The solution would be to model the spread around the trend with an F-distribution, which is what `test="f"` does in `testVar`.
This assumes that the true technical variance for each gene is sampled from an inverse chi-squared distribution, depending on the amount of amplification biases.
In this manner, some protection is provided against the scatter around the trend.
The question is whether there's enough spike-ins to do that precisely, and whether we can assume that the second d.f. is constant over all abundances.
The answer to both questions is probably no, but it's better than nothing - at least there's no obvious defect from discreteness at low abundances that would warrant filtering. 

```{r}
set.seed(100)
nspikes <- 100
ncells <- 100
spike.means <- 2^runif(nspikes, -2, 8)
spike.disp <- (100/spike.means + 0.5) * 10/rchisq(nspikes, df=10)
library(scran)
library(limma)

collected <- list()
for (it in 1:100) {
    spike.data <- matrix(rnbinom(nspikes*ncells, mu=spike.means, size=1/spike.disp), ncol=ncells)
    
    # Fitting the trend
    exprs <- log2(spike.data/(colSums(spike.data)/mean(colSums(spike.data)))+1)
    fit <- trendVar(exprs)
    ab <- fit$mean

    # Estimating the F-distribution
    vals <- fit$var/fit$trend(fit$mean)
    fit.all <- fitFDistRobustly(vals, df1=ncells-1)
    fit.1   <- fitFDistRobustly(vals[ab > 1], df1=ncells-1)
    fit.2   <- fitFDistRobustly(vals[ab > 2], df1=ncells-1)
    fit.4   <- fitFDistRobustly(vals[ab > 4], df1=ncells-1)
    collected[[it]] <- c(fit.all$df2, fit.1$df2, fit.2$df2, fit.4$df2)
}
summary(do.call(rbind, collected))
```

## Sampling distribution of the variance 

The chi-squared assumption above considers normally-distributed observations, where larger values are less precise.
However, the variance estimate for non-normal distributions could be large but more precise than expected, e.g., if there's a consistent number of zeros.
You could get more power by bootstrapping to estimate the sampling distribution, though this is computationally inconvenient.

In practice, it's probably not worth worrying about conservativeness from the sample distribution of the variance estimate.
If this were a problem, the second d.f. would be infinity for the F-distribution (see above) as the sampling variance would be lower than expected under the chi-squared distribution.
This is not the case in most data sets, which suggests that conservativeness is not the problem here.
(Or even if it was, and the first d.f. was effectively underestimated under non-normality, `fitFDistRobustly` would compensate by increasing the second d.f.
This only works to a point, though, because if the second d.f. is already large, further increases will converge to the chi-squared boundary.)

## Comparing biological components between conditions

If you want to compare the size of the components between conditions to find differential variability, it's best to also compare the total:technical variance ratio.
This is because the ratio determines the significance of the HVG detection. 
It's possible to get a gene with a larger component in condition X but with a smaller ratio (and large _p_-value) if the technical variance is large.
Thus, to be sure that a gene is differentially variable, you would need a consistent change in both the absolute and relative value of the biological component.
Otherwise, you can't be sure whether the decrease in precision of the variance estimate is driving an absolute increase in the biological component.
Similarly, if you only used the ratio, you could end up in situations where the ratio increases but the actual variance component does not, which would be nonsensical.

# Correlation comments

## Using HVGs as a pre-screen

Obviously, if you don't pre-screen, you'll get a whole lot of genes that are driven by technical noise.
This should be random and reduce the correlations (and power) -- or, if not random, then definitely uninteresting.

An alternative analytical approach would be a method that detects correlations and HVGs at the same time, where strong correlations would offset low variances for gene detection.
The problem is that, taken to its logical conclusion, this would probably pick up a large web of genes that have strong correlations with low total variances.
This is probably uninteresting, e.g., residual technical effects (like cell size) or uninteresting biology (ribosome-related correlations).

## How to use the correlation results

The idea is to use the correlated gene pairs without having to rely on clustering.
This is closer to the raw data and avoids the errors and ambiguities introduced by clustering.
Negative correlations are particularly powerful as they provide definitive signals for both opposing clusters (or both ends of a trajectory).
This gets around problems in validation where double positive/negative signals might just be due to differences in cell accessibility, permeability, etc.

Of course, relying solely on correlations is also a bit less interpretable, as the identities of the cells in the subpopulations are not explicitly set.
It is also limited to the top set of HVGs, whereas DE between clusters can be checked between all genes.
(Although the rest of the genes are unlikely to have strong DE, otherwise they would have been HVGs.)
Nonetheless, using correlated genes should enrich for structure and reduce the amount of noise going into clustering and dimensionality reduction.

At the very least, one can use the correlation results to back up the (less reliable but more interpretable) higher-level analyses.
So if you get a result from the latter that you mightn't trust (due to uncertainty of clustering, etc.), you can fall back to the correlations if it shows up there.

## Setting an absolute value on the correlation

We could also set a threshold on the absolute value of the correlation.
This is useful when you have lots of cells, which gives you (too much) power to detect non-zero correlations.
It is also valid without further work -- unlike log-fold changes in DE, the absolute correlation here directly determines the p-value.
Thus, you can threshold on the correlation without affecting FDR control, because loss of elements with higher p-values just means the FDR is lower across the rest.
However, I'm disinclined to recommend this explicitly, as you would lose power to detect subtle correlations driving minor subpopulations.
This would defeat the purpose of having lots of cells to improve power to detect those subpopulations.

## Using all HVGs in the brain data set

I also switched to using all HVGs, rather than the top 500 as published.
This is because if you have lots of heterogeneity, genes corresponding to relatively weaker effects are not visible if they get excluded from the top 500.
This occurs even if those effects are actually absolutely large, leading to the inability to detect obvious substructure.
In any case, it actually doesn't take that long, so we might as well just do it using all genes.

## Statistical issues with detecting significant correlations

### Problems with exchangeability

Generation of the null distribution assumes exchangeability of observations.
Specifically, there is the assumption that all observations are equally likely to receive any rank when performing the permutations.
This will not be the case in practice as some observations are more variable than others, depending on the mean-variance relationship.
As such, the variance of the correlations under the null will be underestimated: 

```{r}
means <- rep(c(5, 50), each=50)
disp <- rep(c(1, 0.1), each=50)
counts <- matrix(rnbinom(50000, mu=means, size=1/disp), byrow=TRUE, ncol=length(means))
counts <- t(t(counts)/means)
actual.cor <- cor(t(counts), method="spearman") 
pretend.cor <- correlateNull(100, iters=10000)
var(as.vector(actual.cor))
var(pretend.cor)
testing <- correlatePairs(counts, pretend.cor)
hist(testing$p.value) # fairly substantial loss of type I error control
```

I'm not sure that there's any way to get around this, without making some strong parametric assumptions about how the variance affects the ranking.
I guess we'll just have to suck it up - at least we get some level of protection from spurious correlations.

### Deficiencies with residuals

An obvious approach is to just estimate the correlations between residuals.
However, this is problematic, even in simple one-way layouts.
Consider a situation where you have two groups, with zeroes in almost all cells except for a few.
When you calculate residuals for each gene, you'll get blocks of values corresponding to the zeroes.
The exact value of these blocks with likely differ between groups; this can generate apparent correlations between genes.

```{r}
X <- model.matrix(~rep(LETTERS[1:2], each=50))
g1 <- integer(100)
g1[1] <- 100
g1[51] <- 1000
r1 <- lm.fit(X, g1)$residuals
g2 <- integer(100)
g2[2] <- 200
g2[52] <- 2000
r2 <- lm.fit(X, g2)$residuals
cor(r1, r2, method="spearman")
```

The problem above is why we calculate correlations within each group.
However, this is not possible for complex designs where we need to know the exact effect of each nuisance term on expression (and thus the rank).
Consider a real-valued covariate of 1:100 for 100 cells, where the count for the last cell is unity in each of two genes, and zeroes everywhere else.
You'll get correlations of 1 because the residual effects will be negative for the zeros.
This seems unavoidable; after all, the likelihood of obtaining a zero does change across cells, given the changes in the fitted value you want to correct for.

Don't bother trying to fit a linear model to the ranks, either.
I thought it would be a generalization of the definition of Spearman's (Pearson's on ranks).
However, there's no guarantee that unevenly-spaced covariates (or factors, for that matter) will make sense when fitted to ranks.

# Clustering comments

## Choice of clustering method

Ward's method seems to work well, but complete linkage would also probably do a good job here.
The problem with method selection is that the "best" method depends on the unknown nature of the underlying data.
Ward and complete linkage assume compact clusters, but this might not be the case, e.g., density-based methods would do better for irregular shapes.
This might suggest that ensemble or consensus clustering would perform best.

The issue is with the interpretability of whatever clusters crawl out at the end.
If an assigment only occurs with a minority of methods, should it be discarded, even if those methods focus on particularly pertinent aspects of the data?
This is likely to occur if you use substantially different clustering methods, given that the use of similar methods would defeat the purpose.
Upon summarization, these minority assignments would be discarded and power would be lost relative to application of the minority methods by themselves.

Rather, the main utility of a consensus method is that it tells you which clusters are the most robust with respect to variability from the choice of method.
These clusters can be considered conservative as you need everyone to detect them, resulting in some loss of power.
However, if you assume that each method is affected by noise in different ways, then the consensus clusters are effectively denoised, which is nice.

## Assessing the reliability of clustering

I don't think bootstrapping is appropriate here.
Standard bootstrapping requires IID genes in order to generate bootstrap replicates of the original data.
This is not the case, which makes it difficult to interpret the bootstrap probabilities on an absolute scale.
Doing it correctly would require block resampling to account for correlations -- hence the difficulty.

Using silhouette profiles seems to work pretty well.
The silhouette should be near zero for clusters formed from unstructured data in high dimensions.
This is because the inside/outside distsance will increase with more dimensions, while the distance between adjacent subspaces will stay the same.
As a result, the silhouette value will approach zero.

```{r}
x <- matrix(rnorm(500000), ncol=500) # no real structure
d <- dist(t(x))
out <- kmeans(t(x), centers=3)
require(cluster)
sil <- silhouette(out$cluster, d)
plot(sil)
```

In genomics data with thousands of dimensions, any value above 0.1 is probably fine.
You can also maximize the average silhouette to get the best number of clusters.
This is better than the gap statistic in that it allows you to see the quality of the clusters at the same time.
It's actually advisable to do this, otherwise even well-clustered data will look crap if you overcluster and all the silhouette widths are low.

Another visibility solution is to colour the silhouette by its own colour if positive, and by the colour of its neighbour if negative.
This tells you where the "wrong" cells _should_ have been assigned to.
Thus, a cluster is only well-separated if most its cells have positive width;
and if only a few cells in other clusters have negative widths and have the target cluster as a closest neighbour.

```r
o <- order(sil[,1], sil[,3])
osil <- sil[o,]
ocol <- c("black", "red", "blue")[ifelse(osil[,3] > 0, osil[,1], osil[,2])]
barplot(osil[,3], horiz=TRUE, col=ocol, border=NA)
```

Of course, the gap statistic, silhouette, etc. only check for the separatedness of the clusters.
It doesn't actually know how robust the clusters are to biological/experimental variability, in which case you'd need biological replicates.
The assumption is that well-separated clusters would be more robust to noise, but this really depends on the extent of noise that you'd expect to see.

## Using diffusion maps

Pseudotime coordinates can be extracted for DE analyses with edgeR/DESeq, a la empirical clustering.
This might be more robust than clustering for continuous trajectories where the cluster boundaries would be more or less arbitrary.
Of course, this depends much on the quality of the trajectory reconstruction.

## Visual artifacts in the heatmap

Note that stripes, rather than blocks, are likely to be visual artifacts.
This is because smaller cells have less stable expression and accumulate red/blue colours, while larger cells get more white.
Obviously, if there are problems with normalization, it'll show up here as well.
In general, these can be ignored; if they're not artifacts, then they'll be impossible to validate.

## Identifying marker genes per cluster

### Considering the variability across cells 

DE genes between subpopulations are not necessarily marker genes.
The latter requires that the gene be consistently expressed (or not) in all cells of one subpopulation compared to the other.
This means that the variance needs to be modelled across cells.
For DE genes, any change will do, even if it only appears in a small percentage of cells of the subpopulation.
This requires modelling of the variance across replicate instances of the entire subpopulation.

That being said, testing for differential expression is not the best approach to identify marker genes.
This is because marker genes are defined not just by a change in mean expression, but also low variability.
Just testing for the former doesn't guarantee that it's not variable -- if the difference is strong enough, the null hypothesis will be rejected.
The reasoning above only favours the rejection of genes with low variability across the population, which may or may not improve the ranking of candidate markers.

Simultaneously testing for both criterions might seem like the solution.
However, this raises the question of how much variability is too much, and the desirable trade-off with strong DE.
In the most extreme case, you would look for genes that have the lowest variability.
This would favour genes with weak DE that are poor marker candidates, especially for high-abundance genes expressed in both groups.

```r
set.seed(100)
library(statmod)
ngroup <- 50
design <- model.matrix(~rep(LETTERS[1:2], each=ngroup))
design0 <- cbind(rep(1, nrow(design)))

# Scenario 1: strong difference, highly variable.
# This is arguably a strong candidate for a marker gene.
y1 <- c(integer(ngroup), rnbinom(ngroup, mu=100, size=1)) 
sum(y1==0) - ngroup # only one cell in the second group is zero.
LR1 <- glmnb.fit(design0, y1, dispersion=1, offset=0)$deviance - 
       glmnb.fit(design, y1, dispersion=1, offset=0)$deviance

# Scenario 2: weaker difference, lowly variable.
# This is not a particularly good marker gene.
y2 <- c(rnbinom(ngroup, mu=50, size=100), 
        rnbinom(ngroup, mu=100, size=100)) 
LR2 <- glmnb.fit(design0, y2, dispersion=0.01, offset=0)$deviance - 
       glmnb.fit(design, y2, dispersion=0.01, offset=0)$deviance
 
# Despite that, the second gene has a greater LR than the first.
LR1
LR2
```

In short, whether you sum across cells or model cell-to-cell variability directly, you'll still get bad candidate markers in the rankings.
Strong markers should appear at the top no matter what you do, so perhaps that's reassuring.

### Using _limma_ instead of _edgeR_

Much like in detecting HVGs, applying _limma_ on the log-transformed counts seems to do better than using _edgeR_ on the raw counts.
This is because _edgeR_ puts too much weight on a few highly-expressing cells, which end up driving the DE calls.
(As long as the change in expression is large enough, then it can offset the increase in variability and be detected as strongly DE.)
The log-transformation protects against outliers and seems to yield better rankings, at least for read count data where amplification biases are most prevalent.

Interestingly, direct application of _limma_ seems to be better than `voom` for this purpose.
`voom`'s precision weighting means that only a few cells with large size factors contribute to the DE analysis.
This is because the mean-variance trend is sharply decreasing so a small number of large cells will get very large weights.
As a result, DE genes with high variability across the majority of cells will be highly ranked, because that variability is ignored by the model.

Technically, this is correct as we shouldn't trust variability due to dropouts and other technical noise.
However, if we want to be conservative, then we look for genes that are expressed consistently _in spite of_ the dropouts.
This favours DE genes in which we know that expression is consistent, at the expense of genes where we don't know either way due to dropouts/noise.
To do this, we take the log-expression values at face value and just use _limma_ without weighting.

# Cell cycle phase correction comments

An alternative approach uses genes that have annotated functions in cell cycling and division.
We extract all genes associated with the relevant GO terms and use them to construct a PCA plot for the brain dataset.
Figure ((braincyclepca)) contains three clusters that may correspond to distinct phases of the cell cycle.
This can be determined explicitly by identifying marker genes for each cluster as previously described, and checking whether each marker has known phase-specific expression with resources such as [Cyclebase](http://www.cyclebase.org) [@santos2015cyclebase].

```{r, echo=FALSE, results='hide', message=FALSE, eval=FALSE}
library(org.Mm.eg.db)
fontsize <- theme(axis.text=element_text(size=12), axis.title=element_text(size=16))
```

```{r braincyclepca, fig.width=10, fig.height=5, fig.cap="PCA plot of the brain dataset, using only genes with annotated functions in cell cycling or division.", eval=FALSE}
ccgenes <- select(org.Mm.eg.db, keys=c("GO:0022403", "GO:0051301"), keytype="GOALL", column="SYMBOL")
sce <- readRDS("brain_data.rds")
chosen.genes <- which(rownames(sce) %in% ccgenes$SYMBOL)
plotPCA(sce, feature_set=chosen.genes) + fontsize 
```

We can also identify hidden factors of variation across the annotated genes using `r Biocpkg("RUVSeq")`.
This assumes that, if all cells were in the same phase of the cell cycle, there should be no DE across cells for genes associated with the cell cycle.
Any systematic differences between cells are incorporated into the `W` matrix containing the factors of unwanted variation.
These factors can then be included as covariates in the design matrix to absorb cell cycle effects in the rest of the dataset.
We set `k=2` here to capture the variation corresponding to the two principal components in Figure ((braincyclepca)).

```{r, eval=FALSE}
library(RUVSeq)
ruv.out <- RUVg(exprs(sce), isLog=TRUE, cIdx=chosen.genes, k=2)
head(ruv.out$W)
```

In general, we prefer using the `cyclone`-based approach for phase identification and blocking.
This is because the expression of cell cycle genes may be affected by other biological/experimental factors at the single-cell level.
As a result, the inferred factors of variation may include interesting differences between cells, such that blocking on those factors would result in loss of detection power.
`cyclone` calls the phase for each cell separately and is more robust to systematic (non-cell-cycle-related) differences between cells.

The obvious example would be if cells in the same phase had different amounts of expression for cell cycle genes, corresponding to some other factor (e.g., treatment).
This would get picked up as a hidden factor of variation, leading to loss of power to detect that other factor.
In contrast, cyclone wouldn't care as the relative amounts of expression within each cell would be the same for all cells, leading to correct calling of phase.


